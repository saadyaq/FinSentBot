import json
from kafka import KafkaConsumer, KafkaProducer
import pandas as pd
from config.kafka_config import KAFKA_CONFIG
import re
from nlp.preprocessing import clean_text
from nlp.sentiment_model import SentimentModel
from utils.logger import setup_logger

logger = setup_logger(__name__)

def get_symbol_map() -> dict:
    """
    Construit un dictionnaire qui associe diffÃ©rents alias (ticker, nom complet,
    variantes sans suffixes) Ã  leur ticker officiel du S&PÂ 500. On rÃ©cupÃ¨re la
    liste des entreprises sur WikipÃ©dia et on ajoute quelques alias manuels
    supplÃ©mentaires (Google, Facebook, Tesla, etc.) pour amÃ©liorer la dÃ©tection.
    """
    try:
        logger.info("Building symbol map from S&P 500 companies")
        url = "https://en.wikipedia.org/wiki/List_of_S%26P_500_companies"
        df = pd.read_html(url)[0]
    except Exception as e:
        logger.error(f"Failed to fetch S&P 500 data for symbol mapping: {e}")
        return {}

    symbol_map: dict[str, str] = {}
    # Regex pour supprimer les suffixes courants (INC., CORP., CO., etc.)
    suffix_re = re.compile(
        r"\b(?:INCORPORATED|INC\.?|CORPORATION|CORP\.?|COMPANY|CO\.?|LP|L\.P\.|HOLDING(S)?|GROUP)\b"
    )

    for _, row in df.iterrows():
        symbol = str(row["Symbol"]).upper().strip()
        company = str(row["Security"]).upper().strip()
        # On mappe toujours le ticker vers luiâ€‘mÃªme
        symbol_map[symbol] = symbol
        # On supprime le texte entre parenthÃ¨ses (Â«Â Alphabet Inc. (ClassÂ A)Â Â» â†’ Â«Â Alphabet Inc.Â Â»)
        base = re.sub(r"\s*\(.*?\)", "", company).strip()
        # On gÃ©nÃ¨re plusieurs variantesÂ : nom complet, nom sans parenthÃ¨sesâ€¦
        candidates = {company, base}
        # On retire les suffixes communs
        simplified = suffix_re.sub("", base).strip()
        if simplified:
            candidates.add(simplified)
        # On enlÃ¨ve toute ponctuation et espaces superflus
        candidates = {re.sub(r"[^A-Z0-9 ]", "", c).strip() for c in candidates}
        # Chaque variante est associÃ©e au ticker
        for name in candidates:
            if name and name not in symbol_map:
                symbol_map[name] = symbol

    # Aliases manuels pour les marques populaires
    manual_aliases = {
        "GOOGLE": "GOOGL",
        "FACEBOOK": "META",
        "META": "META",
        "TESLA": "TSLA",
        "NETFLIX": "NFLX",
        "AMAZON": "AMZN",
        "APPLE": "AAPL",
        "MICROSOFT": "MSFT",
        "NVIDIA": "NVDA",
        "ALPHABET": "GOOGL",
        "GOOG": "GOOG",
        "GOOGL": "GOOGL",
        "META PLATFORMS": "META",
        "WALMART": "WMT",
        "UNITEDHEALTH": "UNH",
    }
    for alias, sym in manual_aliases.items():
        symbol_map[alias] = sym
    return symbol_map

def detect_symbol(text: str, symbol_map: dict) -> str | None:
    """
    DÃ©tecte le ticker le plus probable Ã  partir du texte dâ€™un article.
    La recherche sâ€™effectue sur lâ€™ensemble des alias connus via des
    correspondances motâ€‘entier. Les noms les plus longs sont testÃ©s en premier
    pour rÃ©duire les faux positifs (par exemple, Ã©viter que le ticker Â«Â AÂ Â»
    corresponde Ã  nâ€™importe quel mot). Les tickers dâ€™un seul caractÃ¨re sont
    uniquement repÃ©rÃ©s sâ€™ils sont prÃ©cÃ©dÃ©s dâ€™un signe $ ou placÃ©s entre
    parenthÃ¨ses.
    """
    if not text:
        return None
    text_upper = text.upper()
    # On trie les clÃ©s par longueur dÃ©croissante
    for name in sorted(symbol_map.keys(), key=len, reverse=True):
        symbol = symbol_map[name]
        if not name or len(name) <= 1:
            continue
        pattern = rf"\b{re.escape(name)}\b"
        if re.search(pattern, text_upper):
            return symbol
    # Cas particulier pour les tickers dâ€™un seul caractÃ¨re (F, T, Câ€¦)
    for name in sorted(symbol_map.keys(), key=len, reverse=True):
        if len(name) == 1:
            symbol = symbol_map[name]
            if re.search(rf"(\$|\()\s*{re.escape(name)}\s*(\)|\b)", text_upper):
                return symbol
    return None

def main():
    # Charger dynamiquement les symboles S&P500
    SYMBOLS = get_symbol_map()

    consumer = KafkaConsumer(
        KAFKA_CONFIG["topics"]["raw_news"],
        bootstrap_servers=KAFKA_CONFIG["bootstrap_servers"],
        auto_offset_reset='earliest',
        enable_auto_commit=True,
        value_deserializer=lambda m: json.loads(m.decode('utf-8'))
    )

    producer = KafkaProducer(
        bootstrap_servers=KAFKA_CONFIG["bootstrap_servers"],
        value_serializer=lambda v: json.dumps(v).encode('utf-8'),
        **KAFKA_CONFIG["producer_config"]
    )

    model = SentimentModel()
    print("âœ… Sentiment Analysis Consumer is running...")

    for msg in consumer:
        try:
            article = msg.value
            print(f"ğŸ” Received article: {article['title']}")

            cleaned_content = clean_text(article["content"])
            score = model.predict_sentiment(cleaned_content)

            # DÃ©tection automatique du symbole
            symbol = detect_symbol(article.get("content",""), SYMBOLS)

            if not symbol:
                print("â›” Aucun symbole dÃ©tectÃ© dans lâ€™article, article ignorÃ©.")
                continue

            enriched_article = {
                **article,
                "sentiment_score": float(score),
                "symbol": symbol
            }

            producer.send(KAFKA_CONFIG["topics"]["news_sentiment"], value=enriched_article)
            print(f"ğŸ“¤ Sent enriched article with score {score} and symbol {symbol}")

            with open('/home/saadyaq/SE/Python/finsentbot/data/raw/news_sentiment.jsonl', "a") as f:
                f.write(json.dumps(enriched_article) + '\n')

        except Exception as e:
            print(f"âš ï¸ Error processing message: {e}")

if __name__ == "__main__":
    main()
